{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalacja brakujących bibliotek\n",
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install ogb\n",
        "!pip install memory-profiler\n",
        "#!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "#!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "#!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24c5j_LgnSr",
        "outputId": "88e7aac4-4e21-4c25-9fcf-b4246fdf3b5b",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.6.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.12.14)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler) (5.9.5)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio torch-geometric torch-sparse torch-scatter torch-cluster torch-spline-conv\n",
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip uninstall -y torch torchvision torchaudio torch-geometric\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqUV7kJPmX9V",
        "outputId": "f81b6b7c-125c-4589-864a-ec453b36195b",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-spline-conv as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (838.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.3/838.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.5.1+cu118 torchaudio-2.5.1+cu118 torchvision-0.20.1+cu118 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "# !pip install torch-geometric\n"
      ],
      "metadata": {
        "id": "ch0qIMrksRaU",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoohOWdMtyUc",
        "outputId": "d4efc914-d8e4-43e4-e6ee-f0c50e80362b",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu118\n",
            "Uninstalling torch-2.5.1+cu118:\n",
            "  Successfully uninstalled torch-2.5.1+cu118\n",
            "Found existing installation: torchvision 0.20.1+cu118\n",
            "Uninstalling torchvision-0.20.1+cu118:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu118\n",
            "Found existing installation: torchaudio 2.5.1+cu118\n",
            "Uninstalling torchaudio-2.5.1+cu118:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu118\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m589.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (11.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.2)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89991 sha256=c23a26f03df356d905b2e27b2d70f1b609cc0bd1abc755be130f3354c7f5da60\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "Successfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfesO4xQt06o",
        "outputId": "30ced7cf-3976-4bb7-a065-51b438e13ac4",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.6/886.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y dgl\n"
      ],
      "metadata": {
        "id": "ZsSlc0wX0Kc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7de16b9-6caa-4b8e-cbe8-30f23d11a3ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping dgl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl-cu117 -f https://data.dgl.ai/wheels/repo.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktqMRgAr0dPi",
        "outputId": "62f07bcf-82ad-45b2-9f33-549d1df19d55",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu117\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu117-0.9.1.post1-cp310-cp310-manylinux1_x86_64.whl (246.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl-cu117) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl-cu117) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl-cu117) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl-cu117) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl-cu117) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl-cu117) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl-cu117) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl-cu117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl-cu117) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl-cu117) (2024.12.14)\n",
            "Installing collected packages: dgl-cu117\n",
            "Successfully installed dgl-cu117-0.9.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libcusparse11\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TmL-H9800yy",
        "outputId": "1c68081a-bf17-467a-b455-9ef37fa31b64",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcusparse11\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 96.2 MB of archives.\n",
            "After this operation, 216 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 libcusparse11 amd64 11.7.0.107~11.5.1-1ubuntu1 [96.2 MB]\n",
            "Fetched 96.2 MB in 5s (19.2 MB/s)\n",
            "Selecting previously unselected package libcusparse11:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../libcusparse11_11.7.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusparse11:amd64 (11.7.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusparse11:amd64 (11.7.0.107~11.5.1-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "\n",
        "print(\"DGL version:\", dgl.__version__)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aj0_uGu0VTu",
        "outputId": "8ab1db27-f7ca-402c-df92-5d420387ebb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "DGL version: 0.9.1post1\n",
            "PyTorch version: 2.0.1+cu118\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sprawdzenie, czy GPU jest dostępne\n",
        "import torch\n",
        "print(\"Czy GPU jest dostępne:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Używana karta GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYWLMNT9gqf_",
        "outputId": "97548f85-9fc5-4f3f-92d9-fdd7774cc2b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Czy GPU jest dostępne: True\n",
            "Używana karta GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from torch_geometric.data import Data\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eKMw-4JlC68",
        "outputId": "f1afe45b-7a29-4f58-f0ee-e7bd7abf8eb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "60HEJnE5furo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importowanie bibliotek\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, NodePropPredDataset\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import memory_profiler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_undirected\n"
      ],
      "metadata": {
        "id": "ZhTTa-8MNCgo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(library, matrix_format):\n",
        "    dataset_name = \"CiteSeer\"  # Mozna tu zmienić na np. \"PubMed\", \"Cora\", \"CiteSeer\"\n",
        "\n",
        "    if library == \"PyG\":\n",
        "        dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n",
        "        data = dataset[0]\n",
        "\n",
        "        data = convert_pyg_format(data, matrix_format)\n",
        "        return data, dataset_name, dataset\n",
        "\n",
        "    elif library == \"DGL\":\n",
        "        dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n",
        "        data = dataset[0]\n",
        "\n",
        "        graph = dgl.graph((data.edge_index[0], data.edge_index[1]), num_nodes=data.num_nodes)\n",
        "        graph.ndata['feat'] = data.x\n",
        "        graph.ndata['label'] = data.y\n",
        "\n",
        "        graph = convert_dgl_format(graph, matrix_format)\n",
        "\n",
        "        # Ustawianie maski (train_mask, val_mask, test_mask) dla DGL\n",
        "        create_dgl_masks(graph)\n",
        "\n",
        "        return graph, dataset_name, dataset\n",
        "\n",
        "def create_dgl_masks(graph):\n",
        "    #czy graf jest heterogeniczny\n",
        "    if len(graph.ntypes) > 1:  # Mamy więcej niż jeden typ węzłów\n",
        "        for node_type in graph.ntypes:\n",
        "            num_nodes = graph.num_nodes(node_type)\n",
        "            train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "            val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "            test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "            train_mask[:int(0.6 * num_nodes)] = True\n",
        "            val_mask[int(0.6 * num_nodes):int(0.8 * num_nodes)] = True\n",
        "            test_mask[int(0.8 * num_nodes):] = True\n",
        "\n",
        "            graph.nodes[node_type].data['train_mask'] = train_mask\n",
        "            graph.nodes[node_type].data['val_mask'] = val_mask\n",
        "            graph.nodes[node_type].data['test_mask'] = test_mask\n",
        "    else:  # Dla grafu jednorodnego\n",
        "        num_nodes = graph.num_nodes()\n",
        "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "        train_mask[:int(0.6 * num_nodes)] = True\n",
        "        val_mask[int(0.6 * num_nodes):int(0.8 * num_nodes)] = True\n",
        "        test_mask[int(0.8 * num_nodes):] = True\n",
        "\n",
        "        graph.ndata['train_mask'] = train_mask\n",
        "        graph.ndata['val_mask'] = val_mask\n",
        "        graph.ndata['test_mask'] = test_mask"
      ],
      "metadata": {
        "id": "DiI9adbtlD3I"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.transforms as T\n",
        "import dgl\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "def convert_pyg_format(data, matrix_format):\n",
        "    if matrix_format == \"COO\":\n",
        "        # Dane już w formacie COO (edge_index)\n",
        "        return data\n",
        "    elif matrix_format == \"CSR\":\n",
        "        # Konwersja do CSR (SparseTensor)\n",
        "        data.adj_t = SparseTensor(row=data.edge_index[0], col=data.edge_index[1], value=None).to('cpu')\n",
        "        return data\n",
        "    elif matrix_format == \"CSC\":\n",
        "        # Konwersja do CSR, a potem transpozycja do CSC\n",
        "        data.adj_t = SparseTensor(row=data.edge_index[0], col=data.edge_index[1], value=None).to('cpu')\n",
        "        data.adj_t = data.adj_t.t()  # Użycie t() zamiast transpose()\n",
        "        return data\n",
        "    else:\n",
        "        raise ValueError(\"Nieznany format macierzy dla PyG: {}\".format(matrix_format))\n",
        "\n",
        "def convert_dgl_format(graph, matrix_format):\n",
        "    if matrix_format == \"COO\":\n",
        "        # DGL domyślnie obsługuje COO, nie trzeba konwertować\n",
        "        return graph\n",
        "    elif matrix_format == \"CSR\":\n",
        "        # Konwersja do formatu CSR\n",
        "        return dgl.to_block(dgl.to_simple(graph))  # Konwersja na CSR\n",
        "    elif matrix_format == \"CSC\":\n",
        "        graph = dgl.add_self_loop(graph)  # DGL nie wspiera natywnie CSC, dodajemy self-loop\n",
        "        return graph\n",
        "    else:\n",
        "        raise ValueError(\"Nieznany format macierzy dla DGL: {}\".format(matrix_format))\n"
      ],
      "metadata": {
        "id": "A10po7dszwuB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n"
      ],
      "metadata": {
        "id": "IlYModyoBLo0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Sprawdzenie, czy dane są z PyG (PyTorch Geometric)\n",
        "        if hasattr(data, 'x') and (hasattr(data, 'edge_index') or hasattr(data, 'adj_t')):\n",
        "            x = data.x\n",
        "\n",
        "            # Sprawdzenie, czy dane są w formacie CSR (SparseTensor)\n",
        "            if hasattr(data, 'adj_t') and isinstance(data.adj_t, SparseTensor):\n",
        "                edge_index = data.adj_t\n",
        "            else:\n",
        "                edge_index = data.edge_index\n",
        "\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = self.conv2(x, edge_index)\n",
        "\n",
        "        # Sprawdzenie, czy dane są z DGL\n",
        "        elif hasattr(data, 'ndata'):\n",
        "            if isinstance(data.ndata['feat'], dict):\n",
        "                x = data.ndata['feat']['_N']  # Pobieramy tensor dla 'feat'\n",
        "            else:\n",
        "                x = data.ndata['feat']\n",
        "\n",
        "            edge_index = torch.stack(data.edges(), dim=0)\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        if hasattr(data, 'x') and hasattr(data, 'edge_index'):  # PyG\n",
        "            x, edge_index = data.x, data.edge_index\n",
        "        else:  # DGL\n",
        "            if isinstance(data.ndata['feat'], dict):\n",
        "                # Heterogeniczny graf: Pobieramy dane dla typu węzła '_N'\n",
        "                x = data.ndata['feat']['_N']\n",
        "                edge_index = torch.stack(data.edges(), dim=0)\n",
        "            else:\n",
        "                # Graf jednorodny\n",
        "                x = data.ndata['feat']\n",
        "                edge_index = torch.stack(data.edges(), dim=0)\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=1):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        if hasattr(data, 'x') and hasattr(data, 'edge_index'):  # PyG\n",
        "            x, edge_index = data.x, data.edge_index\n",
        "        else:  # DGL\n",
        "            if isinstance(data.ndata['feat'], dict):\n",
        "                x = data.ndata['feat']['_N']\n",
        "            else:\n",
        "                x = data.ndata['feat']\n",
        "\n",
        "            edge_index = torch.stack(data.edges(), dim=0)\n",
        "\n",
        "        # Dla PyG, nadal używamy edge_index z PyG\n",
        "        x = F.relu(self.conv1(x, edge_index))  # Pierwsza warstwa GATConv\n",
        "        x = self.conv2(x, edge_index)          # Druga warstwa GATConv\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ata25VqZtKRv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time"
      ],
      "metadata": {
        "id": "Ev4OX_NnR93X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data):\n",
        "    import torch\n",
        "    import time\n",
        "    import torch.optim as optim\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    model.train()\n",
        "\n",
        "    data = data.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Pomiar pamięci GPU przed treningiem\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(20):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "\n",
        "        # PyG: używamy data.train_mask i data.y\n",
        "        if hasattr(data, 'train_mask'):\n",
        "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        else:\n",
        "            # DGL: Zakładamy, że dane są w jednorodnym formacie\n",
        "            train_mask = data.ndata['train_mask']\n",
        "            label = data.ndata['label']\n",
        "\n",
        "            # Sprawdzenie, czy dict\n",
        "            if isinstance(train_mask, dict):\n",
        "                train_mask = train_mask['_N']\n",
        "                label = label['_N']\n",
        "\n",
        "            loss = F.nll_loss(out[train_mask], label[train_mask])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Pomiar czasu i zużycia pamięci GPU po treningu\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    max_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Pamięć w MB\n",
        "\n",
        "    train_time = end_time - start_time\n",
        "    print(f\"Czas trenowania: {train_time:.2f} s, Maksymalne zużycie pamięci: {max_memory_usage:.2f} MB\")\n",
        "    return train_time, max_memory_usage\n",
        "\n",
        "def evaluate_model(model, data):\n",
        "    import torch\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()  # Tryb ewaluacji\n",
        "    data = data.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        if hasattr(data, 'test_mask'):  # PyG\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct = pred[data.test_mask] == data.y[data.test_mask]\n",
        "            accuracy = int(correct.sum()) / int(data.test_mask.sum())\n",
        "        else:  # DGL\n",
        "            test_mask = data.ndata['test_mask']\n",
        "            label = data.ndata['label']\n",
        "\n",
        "            # Sprawdzenie, czy są dict\n",
        "            if isinstance(test_mask, dict):\n",
        "                test_mask = test_mask['_N']\n",
        "                label = label['_N']\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct = pred[test_mask] == label[test_mask]\n",
        "            accuracy = int(correct.sum()) / int(test_mask.sum())\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "jG6CUKDCtMDp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def run_experiments():\n",
        "    models = {\n",
        "        'GCN': GCN,\n",
        "        'GraphSAGE': GraphSAGE,\n",
        "        'GAT': GAT\n",
        "    }\n",
        "    formats = [\"COO\", \"CSR\", \"CSC\"]\n",
        "    libraries = [\"PyG\", \"DGL\"]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    file_exists = os.path.isfile('experiment_results.csv')\n",
        "\n",
        "    with open('experiment_results.csv', mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Model\", \"Library\", \"Format\", \"Dataset\", \"E2E Time (s)\", \"Training Time (s)\", \"Memory Usage (MB)\", \"Validation Accuracy\"])\n",
        "\n",
        "        for model_name, model_class in models.items():\n",
        "            for lib in libraries:\n",
        "                for fmt in formats:\n",
        "                    start_e2e_time = time.time()  # Start measuring E2E time\n",
        "\n",
        "                    # Load data\n",
        "                    data, dataset_name, dataset = load_data(lib, fmt)\n",
        "                    print(f\"Running experiment for {model_name} with {lib} using {fmt} format on {dataset_name}.\")\n",
        "\n",
        "                    # Initialize model\n",
        "                    if lib == \"PyG\":\n",
        "                        model = model_class(input_dim=data.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
        "                    else:\n",
        "                        feat = data.ndata['feat']\n",
        "                        label = data.ndata['label']\n",
        "\n",
        "                        # Handle dictionary cases\n",
        "                        if isinstance(feat, dict):\n",
        "                            feat = feat['_N']\n",
        "                        if isinstance(label, dict):\n",
        "                            label = label['_N']\n",
        "\n",
        "                        model = model_class(input_dim=feat.shape[1], hidden_dim=16, output_dim=label.max().item() + 1)\n",
        "\n",
        "                    # Measure training time and memory usage\n",
        "                    train_time, mem_usage = train(model, data)\n",
        "\n",
        "                    # Evaluate model\n",
        "                    accuracy = evaluate_model(model, data)\n",
        "\n",
        "                    end_e2e_time = time.time()  # End measuring E2E time\n",
        "                    e2e_time = end_e2e_time - start_e2e_time\n",
        "\n",
        "                    print(f\"E2E time: {e2e_time:.2f}s\")\n",
        "\n",
        "                    # Save results\n",
        "                    writer.writerow([model_name, lib, fmt, dataset_name, e2e_time, train_time, mem_usage, accuracy])\n",
        "                    results.append([model_name, lib, fmt, dataset_name, e2e_time, train_time, mem_usage, accuracy])\n",
        "\n",
        "                    print(f\"Finished {model_name} with {lib} using {fmt} format on {dataset_name}.\")\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "RcJaxCzMTpIg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_experiments()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vteAL_ZxTuMC",
        "outputId": "0e2ce8fb-7c1c-4e0c-b13e-3d12f826d098"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for GCN with PyG using COO format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7904\n",
            "Epoch 6, Loss: 0.5629\n",
            "Epoch 11, Loss: 0.1359\n",
            "Epoch 16, Loss: 0.0381\n",
            "Czas trenowania: 0.07 s, Maksymalne zużycie pamięci: 66.34 MB\n",
            "E2E time: 0.12s\n",
            "Finished GCN with PyG using COO format on CiteSeer.\n",
            "Running experiment for GCN with PyG using CSR format on CiteSeer.\n",
            "Epoch 1, Loss: 1.8017\n",
            "Epoch 6, Loss: 0.7686\n",
            "Epoch 11, Loss: 0.1939\n",
            "Epoch 16, Loss: 0.0543\n",
            "Czas trenowania: 0.12 s, Maksymalne zużycie pamięci: 66.04 MB\n",
            "E2E time: 0.18s\n",
            "Finished GCN with PyG using CSR format on CiteSeer.\n",
            "Running experiment for GCN with PyG using CSC format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7903\n",
            "Epoch 6, Loss: 0.5883\n",
            "Epoch 11, Loss: 0.1297\n",
            "Epoch 16, Loss: 0.0361\n",
            "Czas trenowania: 0.11 s, Maksymalne zużycie pamięci: 66.10 MB\n",
            "E2E time: 0.17s\n",
            "Finished GCN with PyG using CSC format on CiteSeer.\n",
            "Running experiment for GCN with DGL using COO format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7955\n",
            "Epoch 6, Loss: 1.1126\n",
            "Epoch 11, Loss: 0.6932\n",
            "Epoch 16, Loss: 0.4478\n",
            "Czas trenowania: 0.08 s, Maksymalne zużycie pamięci: 66.34 MB\n",
            "E2E time: 0.13s\n",
            "Finished GCN with DGL using COO format on CiteSeer.\n",
            "Running experiment for GCN with DGL using CSR format on CiteSeer.\n",
            "Epoch 1, Loss: 1.8006\n",
            "Epoch 6, Loss: 1.1143\n",
            "Epoch 11, Loss: 0.6924\n",
            "Epoch 16, Loss: 0.4601\n",
            "Czas trenowania: 0.08 s, Maksymalne zużycie pamięci: 65.64 MB\n",
            "E2E time: 0.18s\n",
            "Finished GCN with DGL using CSR format on CiteSeer.\n",
            "Running experiment for GCN with DGL using CSC format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7987\n",
            "Epoch 6, Loss: 1.0399\n",
            "Epoch 11, Loss: 0.5765\n",
            "Epoch 16, Loss: 0.3869\n",
            "Czas trenowania: 0.07 s, Maksymalne zużycie pamięci: 66.39 MB\n",
            "E2E time: 0.13s\n",
            "Finished GCN with DGL using CSC format on CiteSeer.\n",
            "Running experiment for GraphSAGE with PyG using COO format on CiteSeer.\n",
            "Epoch 1, Loss: 1.8092\n",
            "Epoch 6, Loss: 0.2511\n",
            "Epoch 11, Loss: 0.0100\n",
            "Epoch 16, Loss: 0.0009\n",
            "Czas trenowania: 0.15 s, Maksymalne zużycie pamięci: 287.47 MB\n",
            "E2E time: 0.20s\n",
            "Finished GraphSAGE with PyG using COO format on CiteSeer.\n",
            "Running experiment for GraphSAGE with PyG using CSR format on CiteSeer.\n",
            "Epoch 1, Loss: 1.8000\n",
            "Epoch 6, Loss: 0.1452\n",
            "Epoch 11, Loss: 0.0038\n",
            "Epoch 16, Loss: 0.0002\n",
            "Czas trenowania: 0.11 s, Maksymalne zużycie pamięci: 287.61 MB\n",
            "E2E time: 0.17s\n",
            "Finished GraphSAGE with PyG using CSR format on CiteSeer.\n",
            "Running experiment for GraphSAGE with PyG using CSC format on CiteSeer.\n",
            "Epoch 1, Loss: 1.8090\n",
            "Epoch 6, Loss: 0.1414\n",
            "Epoch 11, Loss: 0.0028\n",
            "Epoch 16, Loss: 0.0002\n",
            "Czas trenowania: 0.11 s, Maksymalne zużycie pamięci: 287.68 MB\n",
            "E2E time: 0.17s\n",
            "Finished GraphSAGE with PyG using CSC format on CiteSeer.\n",
            "Running experiment for GraphSAGE with DGL using COO format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7954\n",
            "Epoch 6, Loss: 0.5991\n",
            "Epoch 11, Loss: 0.2204\n",
            "Epoch 16, Loss: 0.0803\n",
            "Czas trenowania: 0.12 s, Maksymalne zużycie pamięci: 287.46 MB\n",
            "E2E time: 0.18s\n",
            "Finished GraphSAGE with DGL using COO format on CiteSeer.\n",
            "Running experiment for GraphSAGE with DGL using CSR format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7996\n",
            "Epoch 6, Loss: 0.6507\n",
            "Epoch 11, Loss: 0.2347\n",
            "Epoch 16, Loss: 0.0880\n",
            "Czas trenowania: 0.13 s, Maksymalne zużycie pamięci: 285.43 MB\n",
            "E2E time: 0.23s\n",
            "Finished GraphSAGE with DGL using CSR format on CiteSeer.\n",
            "Running experiment for GraphSAGE with DGL using CSC format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7988\n",
            "Epoch 6, Loss: 0.6698\n",
            "Epoch 11, Loss: 0.2442\n",
            "Epoch 16, Loss: 0.0905\n",
            "Czas trenowania: 0.14 s, Maksymalne zużycie pamięci: 334.91 MB\n",
            "E2E time: 0.21s\n",
            "Finished GraphSAGE with DGL using CSC format on CiteSeer.\n",
            "Running experiment for GAT with PyG using COO format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7946\n",
            "Epoch 6, Loss: 0.6114\n",
            "Epoch 11, Loss: 0.1151\n",
            "Epoch 16, Loss: 0.0202\n",
            "Czas trenowania: 0.09 s, Maksymalne zużycie pamięci: 67.86 MB\n",
            "E2E time: 0.15s\n",
            "Finished GAT with PyG using COO format on CiteSeer.\n",
            "Running experiment for GAT with PyG using CSR format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7950\n",
            "Epoch 6, Loss: 0.6583\n",
            "Epoch 11, Loss: 0.1583\n",
            "Epoch 16, Loss: 0.0416\n",
            "Czas trenowania: 0.09 s, Maksymalne zużycie pamięci: 68.00 MB\n",
            "E2E time: 0.15s\n",
            "Finished GAT with PyG using CSR format on CiteSeer.\n",
            "Running experiment for GAT with PyG using CSC format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7952\n",
            "Epoch 6, Loss: 0.6759\n",
            "Epoch 11, Loss: 0.1822\n",
            "Epoch 16, Loss: 0.0334\n",
            "Czas trenowania: 0.09 s, Maksymalne zużycie pamięci: 68.07 MB\n",
            "E2E time: 0.15s\n",
            "Finished GAT with PyG using CSC format on CiteSeer.\n",
            "Running experiment for GAT with DGL using COO format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7990\n",
            "Epoch 6, Loss: 1.0220\n",
            "Epoch 11, Loss: 0.5872\n",
            "Epoch 16, Loss: 0.3843\n",
            "Czas trenowania: 0.11 s, Maksymalne zużycie pamięci: 67.71 MB\n",
            "E2E time: 0.17s\n",
            "Finished GAT with DGL using COO format on CiteSeer.\n",
            "Running experiment for GAT with DGL using CSR format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7805\n",
            "Epoch 6, Loss: 0.9438\n",
            "Epoch 11, Loss: 0.5442\n",
            "Epoch 16, Loss: 0.3456\n",
            "Czas trenowania: 0.10 s, Maksymalne zużycie pamięci: 67.01 MB\n",
            "E2E time: 0.20s\n",
            "Finished GAT with DGL using CSR format on CiteSeer.\n",
            "Running experiment for GAT with DGL using CSC format on CiteSeer.\n",
            "Epoch 1, Loss: 1.7857\n",
            "Epoch 6, Loss: 0.8209\n",
            "Epoch 11, Loss: 0.4595\n",
            "Epoch 16, Loss: 0.3070\n",
            "Czas trenowania: 0.10 s, Maksymalne zużycie pamięci: 67.71 MB\n",
            "E2E time: 0.16s\n",
            "Finished GAT with DGL using CSC format on CiteSeer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Przygotowanie danych\n",
        "# library = \"DGL\"  # Możesz zmienić na \"PyG\"\n",
        "# dataset_name = \"Cora\"  # Możesz zmienić na np. \"ogbn-arxiv\", \"CiteSeer\", \"PubMed\"\n",
        "# formats = [\"COO\", \"CSR\", \"CSC\"]  # Format macierzy\n",
        "# models = {\n",
        "#     \"GCN\": GCN,\n",
        "#     \"GraphSAGE\": GraphSAGE,\n",
        "#     \"GAT\": GAT\n",
        "# }\n",
        "\n",
        "# # Iteracja po formatach macierzy\n",
        "# for format_name in formats:\n",
        "#     print(f\"\\n--- Format macierzy: {format_name} ---\")\n",
        "\n",
        "#     # Ładowanie danych\n",
        "#     data, dataset_name, dataset = load_data(library=library, matrix_format=format_name)\n",
        "\n",
        "#     # Trenowanie modeli\n",
        "#     for model_name, model_class in models.items():\n",
        "#         print(f\"\\nRunning experiment for {model_name} with {library} using {format_name} format on {dataset_name}.\")\n",
        "\n",
        "#         # Określenie wymiarów wejściowych i wyjściowych\n",
        "#         input_dim = data.ndata['feat']['_N'].shape[1] if library == \"DGL\" else data.num_features  # Accessing the '_N' node type features\n",
        "#         output_dim = dataset.num_classes\n",
        "\n",
        "#         # Tworzenie modelu\n",
        "#         model = model_class(input_dim=input_dim, hidden_dim=16, output_dim=output_dim)\n",
        "\n",
        "#         # Trenowanie modelu\n",
        "#         train_time, mem_usage = train(model, data)\n",
        "\n",
        "#         # Ewaluacja modelu\n",
        "#         accuracy = evaluate_model(model, data)\n",
        "\n",
        "#         # Wyświetlanie wyników\n",
        "#         print(f\"Finished {model_name} with {library} using {format_name} format on {dataset_name}.\")\n",
        "#         print(f\"Model: {model_name}, Format: {format_name}, Accuracy: {accuracy:.4f}, \"\n",
        "#               f\"Time: {train_time:.2f}s, Memory: {mem_usage:.2f}MB\\n\")\n"
      ],
      "metadata": {
        "id": "K_jMuHjj7b14"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# #data = Planetoid(root='/tmp/Cora', name='Cora', transform=NormalizeFeatures())[0]\n",
        "\n",
        "# # Przygotowanie danych\n",
        "# data, num_classes = load_data(library=\"PyG\", matrix_format=\"COO\", dataset_name=\"Cora\")  # Możesz zmienić zbiór danych (ogbn-arxiv)\n",
        "\n",
        "# # Pętla trenowania modeli\n",
        "# results = []\n",
        "# for model_name, model_class in models.items():\n",
        "#     print(f\"\\nTrenuję model: {model_name}\")\n",
        "#     model = model_class(input_dim=data.num_features, hidden_dim=16, output_dim=num_classes)\n",
        "#     train_time, mem_usage = train(model, data)\n",
        "#     accuracy = evaluate_model(model, data)\n",
        "#     results.append((model_name, train_time, mem_usage, accuracy))\n",
        "#     print(f\"Model: {model_name}, Czas trenowania: {train_time:.2f}s, Zużycie pamięci: {mem_usage:.2f}MB, Dokładność: {accuracy:.4f}\")\n",
        "\n",
        "# # Wyświetlenie wyników\n",
        "# print(\"\\nWyniki:\")\n",
        "# for result in results:\n",
        "#     print(f\"Model: {result[0]}, Czas trenowania: {result[1]:.2f}s, Zużycie pamięci: {result[2]:.2f}MB, Dokładność: {result[3]:.4f}\")\n"
      ],
      "metadata": {
        "id": "gP7Bt7PIuMNT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "li0VGzA_1RUN"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}